# ğŸŒŠ Sel AlanÄ± Segmentasyonu - Ã–dev 5

Bu proje, uydu/hava fotoÄŸraflarÄ± Ã¼zerinden sel basmÄ±ÅŸ alanlarÄ± piksel dÃ¼zeyinde tespit etmeyi (semantic segmentation) amaÃ§lamaktadÄ±r.

## ğŸ“‹ Ä°Ã§erik

- **Veri Seti**: 290 adet havadan Ã§ekilmiÅŸ sel gÃ¶rÃ¼ntÃ¼sÃ¼ ve bunlara ait binary maskeler
- **Problem Tipi**: Semantik Segmentasyon (Piksel dÃ¼zeyinde sÄ±nÄ±flandÄ±rma: Su mu, Kara mÄ±?)
- **KullanÄ±lan Framework**: TensorFlow/Keras

## ğŸ—ï¸ Model Mimarileri

| Model | AÃ§Ä±klama |
|-------|----------|
| **U-Net** | Encoder-Decoder yapÄ±sÄ± ile skip connections. Biyomedikal segmentasyon iÃ§in geliÅŸtirilmiÅŸ klasik mimari. |
| **SegNet** | Encoder-Decoder with pooling indices. SÃ¼rÃ¼cÃ¼sÃ¼z araÃ§ sistemleri iÃ§in optimize edilmiÅŸ. |
| **FPN** | Feature Pyramid Network. Ã‡oklu Ã¶lÃ§ekli Ã¶zellik haritalarÄ± kullanÄ±r. |
| **DeepLabV3+** | Atrous Spatial Pyramid Pooling (ASPP) + Decoder. Google'Ä±n state-of-the-art mimarisi. |
| **EfficientNet-UNet** | Transfer learning ile gÃ¼Ã§lendirilmiÅŸ U-Net. |

## ğŸ“Š DeÄŸerlendirme Metrikleri

- **Dice Coefficient**: F1 Score benzeri, overlap Ã¶lÃ§Ã¼mÃ¼
- **IoU (Jaccard Index)**: Intersection over Union
- **Binary Crossentropy**: Piksel bazlÄ± kayÄ±p
- **Combined Loss**: BCE + Dice Loss kombinasyonu

## ğŸš€ Kurulum

```bash
# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle
pip install -r requirements.txt
```

## ğŸ’» KullanÄ±m

### TÃ¼m Modelleri EÄŸitmek Ä°Ã§in:
```python
python flood_segmentation.py
```

### Tek Model EÄŸitimi (HÄ±zlÄ± Test):
```python
from flood_segmentation import train_single_model

# U-Net modelini 20 epoch eÄŸit
model, results, history = train_single_model('U-Net', epochs=20)
```

### Sadece Belirli Modeli EÄŸitmek:
```python
from flood_segmentation import *

# Veri yÃ¼kle
image_files, mask_files = load_data()
X_train, X_val, X_test, y_train, y_val, y_test = split_data(image_files, mask_files)

# Dataset oluÅŸtur
train_dataset = create_dataset(X_train, y_train, augment_data=True)
val_dataset = create_dataset(X_val, y_val, augment_data=False)
test_dataset = create_dataset(X_test, y_test, augment_data=False)

# Model seÃ§ ve eÄŸit
model = build_deeplabv3plus()  # veya build_unet(), build_segnet(), build_fpn()
model = compile_model(model)
history = train_model(model, train_dataset, val_dataset, 'DeepLabV3+', epochs=30)

# DeÄŸerlendir ve gÃ¶rselleÅŸtir
evaluate_model(model, test_dataset, 'DeepLabV3+')
visualize_predictions(model, test_dataset, 'DeepLabV3+')
```

## ğŸ“ Dosya YapÄ±sÄ±

```
sel Ã¶dev/
â”œâ”€â”€ archive/
â”‚   â”œâ”€â”€ Image/          # Sel gÃ¶rÃ¼ntÃ¼leri (*.jpg)
â”‚   â”œâ”€â”€ Mask/           # Segmentasyon maskeleri (*.png)
â”‚   â””â”€â”€ metadata.csv    # GÃ¶rÃ¼ntÃ¼-mask eÅŸleÅŸtirmesi
â”œâ”€â”€ flood_segmentation.py   # Ana kod
â”œâ”€â”€ requirements.txt        # Gerekli kÃ¼tÃ¼phaneler
â””â”€â”€ README.md              # Bu dosya
```

## ğŸ“ˆ Ã‡Ä±ktÄ±lar

EÄŸitim sonrasÄ± aÅŸaÄŸÄ±daki dosyalar oluÅŸturulur:

- `best_{model_name}.keras` - En iyi model aÄŸÄ±rlÄ±klarÄ±
- `{model_name}_training_history.png` - EÄŸitim grafikleri
- `{model_name}_predictions.png` - Tahmin karÅŸÄ±laÅŸtÄ±rmalarÄ±
- `{model_name}_overlay.png` - Overlay gÃ¶rselleÅŸtirmeler
- `model_comparison.png` - TÃ¼m modellerin karÅŸÄ±laÅŸtÄ±rmasÄ±

## âš™ï¸ KonfigÃ¼rasyon

`Config` sÄ±nÄ±fÄ±ndan parametreleri deÄŸiÅŸtirebilirsiniz:

```python
class Config:
    IMG_HEIGHT = 256      # GÃ¶rÃ¼ntÃ¼ yÃ¼ksekliÄŸi
    IMG_WIDTH = 256       # GÃ¶rÃ¼ntÃ¼ geniÅŸliÄŸi
    BATCH_SIZE = 8        # Batch boyutu
    EPOCHS = 50           # EÄŸitim epoch sayÄ±sÄ±
    LEARNING_RATE = 1e-4  # Ã–ÄŸrenme oranÄ±
    VAL_SPLIT = 0.15      # Validation oranÄ±
    TEST_SPLIT = 0.15     # Test oranÄ±
```

## ğŸ“ Notlar

- GPU kullanÄ±mÄ± Ã¶nerilir (eÄŸitim CPU'da Ã§ok yavaÅŸ olabilir)
- Bellek yetersizliÄŸi durumunda `BATCH_SIZE` deÄŸerini dÃ¼ÅŸÃ¼rÃ¼n
- Veri artÄ±rma (augmentation) eÄŸitim setine otomatik uygulanÄ±r
- Early stopping ile overfitting Ã¶nlenir

## ğŸ‘¤ GeliÅŸtirici

ZÃ¼leyha - GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme Ã–devi 5


